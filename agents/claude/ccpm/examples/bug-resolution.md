# Exemple CCPM : R√©solution de bug syst√®me complexe

> Cas pratique : Investigation et correction d'un bug de performance critique avec CCPM

Cet exemple illustre l'utilisation du framework CCPM pour r√©soudre un bug syst√®me complexe affectant multiple composants, depuis l'investigation initiale jusqu'√† la correction coordonn√©e.

## üö® Contexte du bug

**Sympt√¥mes :** Timeouts al√©atoires sur l'authentification (30% des connexions)  
**Impact :** Utilisateurs ne peuvent pas se connecter  
**Priorit√© :** P0 - Critique  
**SLA :** R√©solution <24h  

**Environnements affect√©s :**
- Production (100% impact)
- Staging (reproduction partielle)
- D√©veloppement (non reproductible)

## Phase 1: Investigation et documentation üîç

### Cr√©ation du PRD d'investigation
```bash
/pm:prd-new auth-timeout-investigation
```

**PRD g√©n√©r√© :**
```markdown
# PRD - Investigation Timeouts Authentification

## üéØ Objectif
Identifier et corriger la cause racine des timeouts d'authentification
affectant 30% des utilisateurs en production.

## üìã Sympt√¥mes observ√©s
- Timeouts apr√®s 30s sur POST /api/auth/login
- Erreur intermittente, pas de pattern clair
- Logs backend normaux, pas d'erreur visible
- Charge syst√®me normale (CPU: 45%, Memory: 60%)

## üîç Hypoth√®ses initiales
1. Contention base de donn√©es sessions
2. Load balancer configuration
3. Connection pool saturation
4. Cache Redis lenteur/indisponibilit√©

## ‚úÖ Crit√®res de r√©solution
- Timeout rate <1% (actuellement 30%)
- Temps de r√©ponse auth <500ms (actuellement 30s)
- Zero impact utilisateur pendant la correction
```

### Conversion en epic technique
```bash
/pm:prd-parse auth-timeout-investigation
```

**Epic d'investigation g√©n√©r√© :**
```markdown
# Epic Technique - Debug Auth Timeouts

## üèóÔ∏è Composants impliqu√©s
- **Frontend :** React auth components, token management
- **Load Balancer :** NGINX config, session affinity  
- **Backend :** Express auth middleware, session handling
- **Database :** PostgreSQL sessions table, connection pool
- **Cache :** Redis session store, connection management
- **Monitoring :** Logs analysis, metrics collection

## üîç Plan d'investigation
1. Monitoring & Observability setup
2. Database performance analysis
3. Redis cache investigation  
4. Load balancer diagnostics
5. Network latency analysis
6. Code review auth flow
```

## Phase 2: D√©composition coordonn√©e üóÇÔ∏è

### Cr√©ation des t√¢ches d'investigation
```bash
/pm:epic-decompose auth-timeout-investigation
```

**Issues g√©n√©r√©es :**
- Issue #2001: Setup enhanced monitoring & alerting
- Issue #2002: Database sessions table analysis  
- Issue #2003: Redis cache performance diagnostics
- Issue #2004: Load balancer logs investigation
- Issue #2005: Network latency measurements
- Issue #2006: Auth middleware code review
- Issue #2007: Frontend timeout handling audit
- Issue #2008: Production fix implementation

### Synchronisation GitHub (mode urgence)
```bash
/pm:epic-sync auth-timeout-investigation
```

**Configuration d'urgence :**
- üö® Labels: `P0-critical`, `bug`, `auth`, `performance`
- üìÖ Milestone: "Hotfix Auth Timeouts"  
- üë• Assign√©s: 4 agents sp√©cialis√©s
- ‚è∞ SLA: 24h r√©solution

## Phase 3: Investigation parall√®le ‚ö°

### Agent Monitoring - Issue #2001
```bash
/pm:issue-start 2001
```

**Enhanced monitoring setup :**
```bash
# Dashboard Grafana custom
query: rate(http_request_duration_seconds{job="auth-service"}[5m])
alert: auth_response_time > 5s

# Logs structur√©s  
tail -f /var/log/auth-service.log | jq '.duration, .endpoint, .status'
```

**D√©couverte :** Pics de latence corr√©l√©s avec certaines adresses IP

### Agent Database - Issue #2002
```bash
/pm:issue-start 2002
```

**Analyse performances :**
```sql
-- Query analysis
SELECT query, calls, mean_time, max_time 
FROM pg_stat_statements 
WHERE query LIKE '%sessions%'
ORDER BY max_time DESC;

-- Index usage
SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read
FROM pg_stat_user_indexes 
WHERE tablename = 'sessions';
```

**D√©couverte :** Index manquant sur `sessions.expires_at` causant des full table scans

### Agent Cache - Issue #2003  
```bash
/pm:issue-start 2003
```

**Diagnostics Redis :**
```bash
# Connexions actives
redis-cli INFO clients | grep connected_clients

# Slow queries
redis-cli SLOWLOG GET 10

# Memory usage
redis-cli INFO memory
```

**D√©couverte :** Connection pool Redis satur√© (max: 10, used: 10)

### Agent Network - Issue #2005
```bash
/pm:issue-start 2005
```

**Latency measurements :**
```bash
# Depuis diff√©rentes zones
for region in us-east us-west eu-central; do
  curl -w "@curl-format.txt" -s -o /dev/null https://api.example.com/health
done
```

**D√©couverte :** Latence normale, pas de probl√®me r√©seau

## Phase 4: Analyse crois√©e et root cause üéØ

### Corr√©lation des d√©couvertes
```bash
# Sync des findings de tous les agents
/pm:issue-sync 2001  # Monitoring insights
/pm:issue-sync 2002  # Database bottleneck  
/pm:issue-sync 2003  # Redis pool saturation
/pm:issue-sync 2005  # Network OK
```

**Root cause identifi√©e :**
```markdown
üéØ ROOT CAUSE ANALYSIS
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PROBL√àME PRINCIPAL: Redis connection pool exhaustion        ‚îÇ
‚îÇ                                                             ‚îÇ  
‚îÇ CAUSE 1: Pool size trop petit (10 connections)             ‚îÇ
‚îÇ CAUSE 2: Sessions non expir√©es correctement                ‚îÇ
‚îÇ CAUSE 3: Index DB manquant ‚Üí queries lentes ‚Üí timeout     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ IMPACT: 30% users affect√©s, cascade de timeouts           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Plan de correction coordonn√©
```bash
/pm:issue-start 2008  # Implementation du fix
```

**Strat√©gie de d√©ploiement :**
1. **Quick win :** Augmenter Redis pool (10‚Üí50)
2. **Medium term :** Ajouter index DB manquant  
3. **Long term :** Cleanup session expiration job

## Phase 5: Impl√©mentation coordonn√©e üîß

### Correction imm√©diate - Redis pool
```javascript
// config/redis.js - Augmentation pool
const redisConfig = {
  host: process.env.REDIS_HOST,
  port: process.env.REDIS_PORT,
  // BEFORE: maxRetriesPerRequest: 10
  maxRetriesPerRequest: 50,
  // BEFORE: lazyConnect: true  
  lazyConnect: false,
  // NEW: Connection pool management
  maxConnections: 50,
  minConnections: 10
};
```

### Correction DB - Index manquant
```sql
-- Migration 2024_01_15_add_sessions_expires_index.sql
CREATE INDEX CONCURRENTLY idx_sessions_expires_at 
ON sessions (expires_at) 
WHERE expires_at > NOW();

-- Vacuum pour optimiser
VACUUM ANALYZE sessions;
```

### Monitoring am√©lior√©
```yaml
# docker-compose.yml - Nouvelles m√©triques
services:
  redis:
    image: redis:7-alpine
    command: redis-server --maxmemory-policy allkeys-lru
    # NOUVEAU: Monitoring Redis
    ports:
      - "6379:6379"
      - "16379:16379"  # Metrics port
```

### Tests de charge
```javascript
// load-test.js - Validation du fix
import http from 'k6/http';

export let options = {
  vus: 100,        // 100 virtual users
  duration: '5m',  // 5 minutes
};

export default function() {
  let response = http.post('https://api.example.com/auth/login', {
    email: 'test@example.com',
    password: 'password123'
  });
  
  check(response, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
}
```

## Phase 6: D√©ploiement et validation üìä

### D√©ploiement par phases
```bash
# 1. Staging deployment
kubectl apply -f k8s/staging/

# 2. Canary deployment (10% traffic)  
kubectl patch deployment auth-service -p '{"spec":{"strategy":{"rollingUpdate":{"maxSurge":"10%"}}}}'

# 3. Full production deployment
kubectl rollout deploy/auth-service
```

### Monitoring post-d√©ploiement
```bash
# M√©triques en temps r√©el
watch -n 5 "curl -s http://monitoring.internal/metrics | grep auth_timeout_rate"

# Logs analysis
tail -f /var/log/auth-service.log | grep -E "(timeout|error)" | wc -l
```

**R√©sultats obtenus :**
```
üìä POST-DEPLOYMENT METRICS (24h apr√®s)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Timeout Rate:      0.3% ‚úÖ (√©tait 30%, objectif <1%)       ‚îÇ
‚îÇ Response Time:     180ms ‚úÖ (√©tait 30s, objectif <500ms)   ‚îÇ  
‚îÇ Error Rate:        0.1% ‚úÖ (√©tait 5%, objectif <0.5%)      ‚îÇ
‚îÇ User Complaints:   0 ‚úÖ (√©tait 150+/jour)                  ‚îÇ
‚îÇ Redis Pool Usage:  45% ‚úÖ (√©tait 100%, healthy <80%)       ‚îÇ  
‚îÇ DB Query Time:     15ms ‚úÖ (√©tait 2.5s, objectif <100ms)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Cl√¥ture coordonn√©e des issues
```bash
# Validation et cl√¥ture progressive
/pm:issue-close 2001  # Monitoring setup ‚úÖ
/pm:issue-close 2002  # DB analysis & index ‚úÖ  
/pm:issue-close 2003  # Redis pool fix ‚úÖ
/pm:issue-close 2004  # Load balancer OK ‚úÖ
/pm:issue-close 2005  # Network analysis OK ‚úÖ
/pm:issue-close 2006  # Code review OK ‚úÖ
/pm:issue-close 2007  # Frontend handling OK ‚úÖ
/pm:issue-close 2008  # Production fix deployed ‚úÖ
```

## üìà M√©triques de r√©solution

### Performance de l'√©quipe
| M√©trique | Valeur | Commentaire |
|----------|--------|-------------|
| **Temps total** | 18h | Objectif <24h ‚úÖ |
| **Time to detection** | 2h | Monitoring am√©lior√© |  
| **Time to diagnosis** | 8h | Investigation parall√®le |
| **Time to fix** | 6h | D√©ploiement coordonn√© |
| **Time to validation** | 2h | Tests automatis√©s |

### Impact business
| M√©trique | Avant | Apr√®s | Am√©lioration |
|----------|-------|-------|--------------|
| **Login Success Rate** | 70% | 99.7% | +29.7% |
| **User Satisfaction** | 2.1/5 | 4.8/5 | +128% |  
| **Revenue Loss** | $50K/day | $0 | 100% |
| **Support Tickets** | 150/day | 2/day | -99% |

## üéØ Le√ßons apprises

### ‚úÖ Ce qui a bien fonctionn√© avec CCPM

#### Investigation parall√®le efficace
- **4 agents** enqu√™tant simultan√©ment sur diff√©rentes couches
- **Zero duplication** d'effort gr√¢ce au worktree isolation
- **Sync quotidien** permettant la corr√©lation des d√©couvertes
- **GitHub Issues** centralisant toutes les d√©couvertes

#### Root cause analysis acc√©l√©r√©e  
- **D√©composition syst√©matique** : chaque composant investigu√©
- **Tra√ßabilit√© compl√®te** : du sympt√¥me √† la cause racine
- **Documentation auto-g√©n√©r√©e** : r√©utilisable pour futurs bugs
- **Coordination naturelle** : agents travaillant vers un objectif commun

#### D√©ploiement risk-free
- **Tests parall√®les** pendant le d√©veloppement du fix
- **Validation multi-environnement** avant production
- **Rollback plan** document√© automatiquement
- **Monitoring proactif** post-d√©ploiement

### üìà Am√©liorations identifi√©es

#### Pr√©vention future
1. **Monitoring proactif** : Alerts sur pool utilization >80%
2. **Load testing** : Int√©gration continue des tests de charge  
3. **Capacity planning** : Review trimestrielle des ressources
4. **Runbooks** : Documentation automatique des r√©solutions

#### Process optimisation  
1. **Templates PRD bug** : Formats pr√©d√©finis pour investigation
2. **Escalation automatique** : Triggers sur SLA breach
3. **Post-mortem framework** : Analyse syst√©matique apr√®s incident
4. **Knowledge base** : Catalogue des patterns de bugs r√©solus

## üîÑ Impact long terme

### Pr√©vention mise en place
```bash
# Monitoring proactif d√©ploy√©
/pm:prd-new proactive-monitoring-system

# Capacity planning automatis√©  
/pm:prd-new auto-scaling-policies

# Runbook g√©n√©ration
/pm:prd-new incident-response-automation
```

### M√©triques de fiabilit√© (6 mois apr√®s)
- **MTTR (Mean Time To Resolution)** : 18h ‚Üí 3h (-83%)
- **MTBF (Mean Time Between Failures)** : 2 semaines ‚Üí 3 mois (+600%)
- **Incident recurrence** : 40% ‚Üí 5% (-88%)
- **Team confidence** : 6.2/10 ‚Üí 9.1/10 (+47%)

## üìö Ressources cr√©√©es

### Documentation g√©n√©r√©e
1. **[Runbook Auth Timeouts](./runbooks/auth-timeouts.md)** - Guide de r√©solution
2. **[Monitoring Playbook](./monitoring/auth-monitoring.md)** - Setup alerting  
3. **[Load Testing Guide](./testing/auth-load-tests.md)** - Tests de performance
4. **[Post-mortem Report](./incidents/auth-timeout-postmortem.md)** - Analyse compl√®te

### Templates r√©utilisables
1. **[Bug Investigation PRD](./templates/bug-investigation-prd.md)** - Template investigation
2. **[Incident Response Epic](./templates/incident-response-epic.md)** - Workflow urgence
3. **[Performance Issue Checklist](./checklists/performance-debug.md)** - Checklist debug

---

> üí° **Conclusion :** Cette r√©solution d√©montre l'efficacit√© du framework CCPM pour les incidents critiques. L'investigation parall√®le, la coordination via GitHub Issues et la documentation automatique ont permis de r√©soudre un bug complexe en 18h au lieu des 3-5 jours habituels, tout en cr√©ant de la valeur pour les futurs incidents similaires.